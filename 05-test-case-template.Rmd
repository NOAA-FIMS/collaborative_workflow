# Test case template

In this section we will describe how to write a test case for your FIMS code.

## Introduction

FIMS testing framework will include different types of testing to make sure that changes to FIMS code are working as expected. The unit and functional tests will be developed during the initial development stage when writing individual functions or modules. After completing development of multiple modules, integration testing will be developed to verify that different modules work well together. Checks will be added in the software to catch user input errors when conducting run-time testing. Regression testing and platform compatibility testing will be executed before pre-releasing FIMS. Beta-testing will be used to gather feedback from users (i.e., members of FIMS implementation team and other users) during the pre-release stage. After releasing the first version of FIMS, the development team will go back to the beginning of the testing cycle and write unit tests when a new feature needs to be implemented. One-off testing will be used for testing new features and fixing user-reported bugs when maintaining FIMS. More details of each type of test can be found in the Glossary section. 

FIMS will use GoogleTest to build a C++ unit testing framework and R testthat to build an R testing framework. FIMS will use Google Benchmark to measure the real time and CPU time used for running the produced binaries.

## C++ unit testing and benchmarking 

### Requirements

To use GoogleTest, you will need:

-   A compatible operating system (e.g. Windows, masOS, or Linux).

-   A C++ compiler that supports at least C++ 11 standard or newer (e.g. gcc 5.0+, clang 5.0+, or MSVC 2015+). For macOS users, Xcode 9.3+ provides clang 5.0.

-   A build system for building the testing project. [[CMake]{.ul}](https://cmake.org/) and a compatible build tool such as [[Ninja]{.ul}](https://ninja-build.org/) are approved softwares by NMFS HQ.

### Quickstart for Windows user

-   Download [[CMake 3.22.1 (cmake-3.22.1-windows-x86_64.zip)]{.ul}](https://github.com/Kitware/CMake/releases) and put the file folder to `Documents\Apps` or other preferred folder.

-   Download [[ninja v1.10.2 (ninja-win.zip)]{.ul}](https://github.com/ninja-build/ninja/releases) and put the application to `Documents\Apps` or other preferred folder.

-   Search `Edit environment variables for your account` and open the `Environment Variables` window.

-   Click `Edit...` under the `User variables for firstname.lastname` section.

-   Click `New`, add path to `cmake-3.22.1-windows-x86_64\bin`, add click `OK`.

-   Click `New`, add path to `Documents\Apps`, add click `OK`.

-   Open your Command Prompt and type `cmake`. If you see details of usage, you install the build system successfully.

-   See [[CMake installation instructions]{.ul}](https://cmake.org/install) for installing CMake on other platforms.

### Set up FIMS testing project
-   Go to the FIMS C++ tests folder and create a `CMakeLists.txt` file. Declare a dependency on GoogleTest with following contents: 

```cmake
cmake_minimum_required(VERSION 3.14)
project(FIMS_project)

# GoogleTest requires at least C++11
set(CMAKE_CXX_STANDARD 11)

include(FetchContent)
FetchContent_Declare(
  googletest
  URL https://github.com/google/googletest/archive/refs/tags/release-1.11.0.zip
)

# For Windows: Prevent overriding the parent project's compiler/linker settings
set(gtest_force_shared_crt ON CACHE BOOL "" FORCE)
FetchContent_MakeAvailable(googletest)
```
-   If a `CMakeLists.txt` file already exists in the tests folder, you can start creating a unit test and add it to the `CMakeLists.txt`.  

### Unit test template

```c
#include "gtest/gtest.h"
#include "../src/code.hpp"

// # R code that generates true values for the test

namespace {

  // Description of Test 1 
  TEST(TestSuiteName, Test1Name) {
    
    ... test body ... 
    
  }
  
  // Description of Test 2
  TEST(TestSuiteName, Test2Name) {
    
    ... test body ...
    
  }
  
}

```

### Unit test example

Let's create dlognorm.hpp that has a simple function:

```c
#include <cmath>

template<class Type>
Type dlognorm(Type x, Type meanlog, Type sdlog){
  Type resid = (log(x)-meanlog)/sdlog;
  Type logres = -log(sqrt(2*M_PI)) - log(sdlog) - Type(0.5)*resid*resid - log(x);
  return logres; 
}
```

We can create a test file dlognorm-unit.cpp that has a test suite for this function:

```c
#include "gtest/gtest.h"
#include "../src/dlognorm.hpp"

// # R code that generates true values for the test
// dlnorm(1.0, 0.0, 1.0, TRUE) = -0.9189385
// dlnorm(5.0, 10.0, 2.5, TRUE) = -9.07679

namespace {

  // TestSuiteName: dlognormTest; TestName: DoubleInput and IntInput
  // Test dlognorm with double input values
  
  TEST(dlognormTest, DoubleInput) {
    
    EXPECT_NEAR( dlognorm(1.0, 0.0, 1.0) , -0.9189385 , 0.0001 ); 
    EXPECT_NEAR( dlognorm(5.0, 10.0, 2.5) , -9.07679 , 0.0001 ); 
    
  }
  
  // Test dlognorm with integer input values
  
  TEST(dlognormTest, IntInput) {
    
    EXPECT_NE( dlognorm(1, 0, 1) , -0.9189385 );
    
  }
  
}
```

`EXPECT_NEAR(val1, val2, absolute_error)` verifies that the difference between `val1` and `val2` does not exceed the absolute error bound `absolute_error`. `EXPECT_NE(val1, val2)` verifies that `val1` is not equal to `val2`. Please see GoogleTest [assertions reference](https://google.github.io/googletest/reference/assertions.html) for more `EXPECT_` macros. 

### Add tests to `CMakeLists.txt` and run a binary

To build the code, add the following contents to the end of your `CMakeLists.txt` file:

```cmake
enable_testing()

add_executable(
  dlognorm_test
  dlognorm-unit.cpp
)

target_include_directories(
  dlognorm_test PUBLIC
  ${CMAKE_SOURCE_DIR}/../
)

target_link_libraries(
  dlognorm_test
  gtest_main
)

include(GoogleTest)
gtest_discover_tests(dlognorm_test)
```

The above configuration enables testing in CMake, declares the C++ test binary you want to build (dlognorm_test), and links it to GoogleTest (gtest_main). Now you can build and run your test:

```bash
cd tests
cmake -S . -B build -G Ninja
cd build 
cmake --build .
ctest
```
The output might look like this:

```bash
Start 1: dlognormTest.DoubleInput
1/2 Test #1: dlognormTest.DoubleInput .........   Passed    0.11 sec
Start 2: dlognormTest.IntInput
2/2 Test #2: dlognormTest.IntInput ............   Passed    0.11 sec

100% tests passed, 0 tests failed out of 2

Total Test time (real) =   0.25 sec
```
Congratulations! You’ve successfully set up a test project and run a test binary using GoogleTest. Let's use Google Benchmark to measure the real time and CPU time used for running the produced binary.

### Benchmark template

```c
#include "benchmark/benchmark.h"
#include "../src/code.hpp"

void BM_FunctionName(benchmark::State& state)
{
  for (auto _ : state)
    // This code gets timed
    Function()
}

// Register the function as a benchmark
BENCHMARK(BM_FunctionName);

```

### Benchmark example

We will continue using the dlognorm.hpp example. We can create a benchmark file dlognorm_benchmark.cpp and put it in the tests folder:

```c
#include "benchmark/benchmark.h"
#include "../src/dlognorm.hpp"

void BM_dlgnorm(benchmark::State& state)
{
  for (auto _ : state)
    dlognorm(5.0, 10.0, 2.5);
}
BENCHMARK(BM_dlgnorm);

```
Please see more examples on [Google Benchmark GitHub repository](https://github.com/google/benchmark) for a more comprehensive feature overview.

### Add benchmarks to `CMakeLists.txt` and run the benchmark

To build the code, add the following contents to the end of your `CMakeLists.txt` file:

```cmake

FetchContent_Declare(
  googlebenchmark
  URL https://github.com/google/benchmark/archive/refs/tags/v1.6.0.zip
)
FetchContent_MakeAvailable(googlebenchmark)

add_executable(
  dlognorm_benchmark
  dlognorm_benchmark.cpp
)

target_include_directories(
  dlognorm_benchmark PUBLIC
  ${CMAKE_SOURCE_DIR}/../
)

target_link_libraries(
  dlognorm_benchmark
  benchmark_main
)

```

To run the benchmark, 

```bash
cmake --build .
./dlognorm_benchmark.exe
```
The output might look like this:

```bash

Run on (8 X 2112 MHz CPU s)
CPU Caches:
  L1 Data 32 KiB (x4)
L1 Instruction 32 KiB (x4)
L2 Unified 256 KiB (x4)
L3 Unified 8192 KiB (x1)
***WARNING*** Library was built as DEBUG. Timings may be affected.
-----------------------------------------------------
  Benchmark           Time             CPU   Iterations
-----------------------------------------------------
  BM_dlgnorm        153 ns          153 ns      4480000
```

## R testing

FIMS uses R testthat package for writing R tests. You can install the packages following the instructions on [testthat website](https://testthat.r-lib.org/). If you are not familiar with testthat, the [testing chapter](https://r-pkgs.org/tests.html) in R packages gives a good overview of testing workflow, along with structure explanation and concrete examples.

### R testthat template

```r
test_that("TestName", {
  
  ...test body...
  
})

```

## Test case template and examples

### Test case template

Individual functional or integration test cases will be designed following the template below.

-   *Test ID*. Create a meaningful name for the test case.

-   *Features to be tested*. Provide a brief statement of test objectives and description of the features to be tested. (Identify the test items following the [[FIMS software design specification document]{.ul}](https://docs.google.com/document/d/1iSEhJqcpSD269QdABeDE4aBZGqGcBrIrLnS7eMkSYv0/edit?usp=sharing) and identify all features that will not be tested and the rationale for exclusion)

-   *Approach*. Specify the approach that will ensure that the features are adequately tested and specify which type of test is used in this case.

-   *Evaluation criteria*. Provide a list of expected results and acceptance criteria.

    -   Pass/fail criteria. Specify the criteria used to determine whether each feature has passed or failed testing.

    -   In addition to setting pass/fail criteria with specific tolerance values, a documentation that just views the outputs of some tests may be useful if the tests require additional computations, simulations, and comparisons

-   *Test deliverables*. Identify all information that is to be delivered by the test activity.

    -   Test logs and automated status reports

### Test case examples

#### General test case

The test case below is a general case and it can be applied to many functions/modules. For individual functions/modules, please make detailed test cases for specific options to avoid duplication as much as possible.

+-----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+
| Test ID               | General test case                                                                                                                                   |
+=======================+=====================================================================================================================================================+
| Features to be tested | -   The function/module returns correct output values given different input values                                                                  |
|                       |                                                                                                                                                     |
|                       | -   The function/module returns error messages when users give wrong types of inputs                                                                |
|                       |                                                                                                                                                     |
|                       | -   The function/module notifies an error if the input value is outside the bound of the input parameter                                            |
+-----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+
| Approach              | -   Prepare expected true values using R                                                                                                            |
|                       |                                                                                                                                                     |
|                       | -   Run tests in R using testthat and compare output values with expected values                                                                    |
|                       |                                                                                                                                                     |
|                       | -   Push tests to the working repository and run tests using GitHub Actions                                                                         |
|                       |                                                                                                                                                     |
|                       | -   Run tests in different OS environments (windows latest, macOS latest, and ubuntu latest) using GitHub Actions                                   |
|                       |                                                                                                                                                     |
|                       | -   Submit pull request for code review                                                                                                             |
+-----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+
| Evaluation Criteria   | -   The tests pass if the output values equal to the expected true values                                                                           |
|                       |                                                                                                                                                     |
|                       | -   The tests pass if the function/module returns error messages when users give wrong types of inputs                                              |
|                       |                                                                                                                                                     |
|                       | -   The tests pass if the function/module returns error messages when user provides an input value that is outside the bound of the input parameter |
+-----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+
| Test deliverables     | -   Test logs on GitHub Actions                                                                                                                     |
+-----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+

#### Functional test example: TMB probability mass function of the multinomial distribution 

+-----------------------+----------------------------------------------------------------------------------+
| Test ID               | Probability mass function of the multinomial distribution                        |
+=======================+==================================================================================+
| Features to be tested | -   Same as the general test case                                                |
+-----------------------+----------------------------------------------------------------------------------+
| Approach              | Functional test                                                                  |
|                       |                                                                                  |
|                       | -   Prepare expected true values using R function dmultinom from package 'stats' |
+-----------------------+----------------------------------------------------------------------------------+
| Evaluation Criteria   | -   Same as the general test case                                                |
+-----------------------+----------------------------------------------------------------------------------+
| Test deliverables     | -   Same as the general test case                                                |
+-----------------------+----------------------------------------------------------------------------------+

#### Integration test example: [[Li et al. 2021 age-structured stock assessment model comparison]{.ul}](https://doi.org/10.7755/FB.119.2-3.5) 

+-----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Test ID               | Age-structured stock assessment comparison ([[Li et al. 2021]{.ul}](https://doi.org/10.7755/FB.119.2-3.5))                                                                                                                                     |
+=======================+================================================================================================================================================================================================================================================+
| Features to be tested | -   Null case (update standard deviation of the log of recruitment from 0.2 to 0.5 based on [[Siegfried et al. 2016]{.ul}](http://dx.doi.org/10.1139/cjfas-2015-0398) snapper-grouper complex)                                                 |
|                       |                                                                                                                                                                                                                                                |
|                       | -   Recruitment variability                                                                                                                                                                                                                    |
|                       |                                                                                                                                                                                                                                                |
|                       | -   Stochastic Fishing mortality (F)                                                                                                                                                                                                           |
|                       |                                                                                                                                                                                                                                                |
|                       | -   F patterns (e.g., roller coaster: up then down and down then up; constant F~low~, F~MSY~, and F~high~)                                                                                                                                     |
|                       |                                                                                                                                                                                                                                                |
|                       | -   Selectivity patterns                                                                                                                                                                                                                       |
|                       |                                                                                                                                                                                                                                                |
|                       | -   Recruitment bias adjustment                                                                                                                                                                                                                |
|                       |                                                                                                                                                                                                                                                |
|                       | -   Initial condition                                                                                                                                                                                                                          |
|                       |                                                                                                                                                                                                                                                |
|                       | -   (unit of catch: number or weight)                                                                                                                                                                                                          |
|                       |                                                                                                                                                                                                                                                |
|                       | -   Model misspecification (e.g., growth, natural mortality, and steepness, catchability etc)                                                                                                                                                  |
+-----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Approach              | Integration test                                                                                                                                                                                                                               |
|                       |                                                                                                                                                                                                                                                |
|                       | -   Prepare expected true values from an operating model using R functions from [[Age_Structured_Stock_Assessment_Model_Comparison]{.ul}](https://github.com/Bai-Li-NOAA/Age_Structured_Stock_Assessment_Model_Comparison) GitHub repository   |
+-----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Evaluation Criteria   | -   Summarize median absolute relative error (MARE) between true values from the operating model and the FIMS estimation model                                                                                                                 |
|                       |                                                                                                                                                                                                                                                |
|                       | -   If all MAREs from the null case are less than 10% and all MARES are less than 15%, the tests pass. If the MAREs are greater than 15%, a closer examination is needed.                                                                      |
+-----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Test deliverables     | -   In addition to the test logs on GitHub Actions, a document that includes comparison figures from various cases (e.g., Fig 5 and 6 from Li et al. 2021) will be automatically generated                                                     |
|                       |                                                                                                                                                                                                                                                |
|                       | -   A table that shows median absolute relative errors in unfished recruitment, catchability, spawning stock biomass, recruitment, fishing mortality, and reference points (e.g., Table 6 from Li et al. 2021) will be automatically generated |
+-----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

#### simulation testing: challenges and solutions

One thing that might be challenging for comparing simulation results is that changes to the order of different calls to simulation will change the simulated values and then tests may fail even though it is just because different random numbers are used or the order of the simulation changes through model development. Several solutions could be used to address the simulation testing issue. Please see [discussions](https://github.com/NOAA-FIMS/FIMS-planning/issues/25) on the FIMS-planning issue page for details.  

- Once we start developing simulation modules, there are two ways that help compare simulated data from FIMS and a test.
  - Add a TRUE/FALSE parameter in each FIMS simulation module for setting up testing seed. When testing the module, use the parameter=TRUE to fix the seed number in R and conduct tests. 
  - If adding a TRUE/FALSE parameter does not work as expected, then carefully check simulated data from each component and make sure it is not a model coding error. 
   
- FIMS will use set.seed() from R to set seed. “rstream” package will be investigated if one of the requirements of FIMS simulation module is to generate multiple streams of random numbers to associate distinct streams of random numbers with different sources of randomness. rstream was specifically designed to address the issue of needing very long streams of pseudo-random numbers for parallel computations. Please see [rstream paper](https://www.iro.umontreal.ca/~lecuyer/myftp/papers/rstream.pdf) and [RngStreams](http://www-labs.iro.umontreal.ca/~lecuyer/myftp/streams00/) for more details.

## Glossary

### Unit testing

-   Description: It tests individual methods and functions of the classes, components or modules used by the software independently. It executes only small portions of the test cases during the development process.

-   Writer: Developer

-   Advantages: It finds problems early and helps trace the bugs in the development cycle; cheap to automate when a method has clear input parameters and output; can be run quickly.

-   Limitations: Tedious to create; it won't catch integration errors if a method or a function has interactions with something external to the software.

-   Examples: A recruitment module may consist of a few stock-recruit functions. We could use a set of unit test cases that ensure each stock-recruit function is correct and meets its design as intended while developing the function.

-   Reference: [[Wikipedia description]{.ul}](https://en.wikipedia.org/wiki/Unit_testing)

### Functional testing

-   Description: It checks software's performance with respect to its specified requirements. Testers do not need to examine the internal structure of the piece of software tested but just test a slice of functionality of the whole system after it has been developed.

-   Writer: Tester

-   Advantages: It verifies that the functionalities of the software are working as defined; lead to reduced developer bias since the tester has not been involved in the software's development.

-   Limitations: Need to create input data and determine output based on each function's specifications; need to know how to compare actual and expected outputs and how to check whether the software works as the requirements specified.

-   Examples: The software requires development of catch-based projection. We could use a set of functional test cases that help verify if the model produces correct output given specified catch input after catch-based projection has been implemented in the system.

-   Reference: [[Wikipedia description]{.ul}](https://en.wikipedia.org/wiki/Functional_testing); [[WHAM testthat examples]{.ul}](https://github.com/timjmiller/wham/tree/master/tests/testthat)

### Integration testing

-   Description: A group of software modules are coupled together and tested. Integrate software modules all together and verify the interfaces between modules against the software design. It is tested until the software works as a system.

-   Writer: Tester

-   Advantages: It builds a working version of the system by putting the modules together. It assembles a software system and helps detect errors associated with interfacing.

-   Limitations: The tests only can be executed after all the modules are developed. It may be difficult to locate errors because all components are integrated together.

-   Examples: After developing all the modules, we could set up a few stock assessment test models and check if the software can read the input file, run the stock assessment models, and provide desired output.

-   Reference: [[Wikipedia description]{.ul}](https://en.wikipedia.org/wiki/Integration_testing)

### Run-time testing

-   Description: Checks added in the software that catch user input errors. The developer will add in checks to the software; the user will trigger these checks if there are input errors

-   Writer: developer

-   Advantages: Provides guidance to the user while using the software

-   Limitations: Adding many checks can cause the software to run more slowly, the messages need to be helpful so the user can fix the input error.

-   Examples: A user inputs a vector of values when they only need to input a single integer value. When running the software, they get an error message telling them that they should use a single integer value instead.

-   Reference: [[Testing R code book]{.ul}](http://www.crcpress.com/9781498763653)

### Regression testing

-   Description: Re-running tests to ensure that previously developed and tested software still performs after a change. Testers can execute regression testing after adding a new feature to the software or whenever a previously discovered issue has been fixed. Testers can run all tests or a part of the test suite to check the correctness or quality of the software.

-   Writer: Tester

-   Advantages: It ensures that the changes made to the software have not affected the existing functionalities or correctness of the software.

-   Limitations: If the team makes changes to the software often, it may be difficult to run all tests from the test suite frequently. In that case, it's a good idea to have a regression testing schedule. For example, run a part of the test suite that is higher in priority after every change and run the full test suite weekly or monthly, etc.

-   Examples: Set up a test suit like the [[ss-test-models]{.ul}](https://github.com/nmfs-stock-synthesis/ss-test-models) repository. The test cases can be based on real stock assessment models, but may not be the final model version or may have been altered for testing purposes. Test the final software by running this set of models and seeing if the same results for key model quantities remain the same relative to a "reference run" (e.g., the last release of the software).

-   Reference: [[Wikipedia description]{.ul}](https://en.wikipedia.org/wiki/Regression_testing)

### Platform compatibility testing

-   Description: It checks whether the software is capable of running on different operating systems and versions of other softwares. Testers need to define a set of environments or platforms the application is expected to work on. Testers can test the software on different operating systems or platforms and report the bugs.

-   Writer: Tester

-   Advantages: It ensures that the developed software works under different configurations and is compatible with the client's environment.

-   Limitations: Testers need to have knowledge of the testing environment and platforms to understand the expected software behavior under different configurations. It may be difficult to figure out why the software produces different results when using different operating systems.

-   Examples: Set up an automated workflow and see if the software can be compatible with different operating systems, such as Windows, macOS, and Linux. Also, testers can check if the software is compatible with different versions of R (e.g., release version and version 3.6, etc).

-   Reference: [[International Software Testing Qualification Board]{.ul}](https://glossary.istqb.org/en/search/compatibility)

### Beta testing

-   Description: It is a form of external user acceptance testing and the feedback from users can ensure the software has fewer bugs. The software is released to a limited end-users outside of the implementation team and the end-users (beta testers) can report issues of beta software to the implementation team after further testing.

-   Writer: Members of implementation team and other users

-   Advantages: It helps in uncovering unexpected errors that happen in the client's environment. The implementation team can receive direct feedback from users before shipping the software to users.

-   Limitations: The testing environment is not under the control of the implementation team and it may be hard to reproduce the bugs.

-   Examples: Prepare a document that describes the new features of the software and share it with selected end-users. Send a pre-release of the software to selected users for further testing and gather feedback from users.

-   Reference: [[Wikipedia description]{.ul}](https://en.wikipedia.org/wiki/Software_testing); [[SS prerelease example]{.ul}](https://github.com/nmfs-stock-synthesis/stock-synthesis/releases/tag/v3.30.18-prerel)

### One-off testing

-   Description: It is for replicating and fixing user-reported bugs. It is a special testing that needs to be completed outside of the ordinary routine. Testers write a test that replicates the bug and run the test to check if the test is failing as expected. After fixing the bug, the testers can run the test again and check if the test is passing.

-   Writer: Developer and tester

-   Advantages: The test is simple, fast, and efficient for fixing bugs.

-   Limitations: The tests are specific to bugs and may require manual testing.

-   Examples: A bug is found in the code and the software does not work properly. Tester can create a test to replicate the bug and the test would fail as expected. After the developer fixes the bug, the tester can run the test and see if the issue is resolved.

-   Reference: [[International Software Testing Qualification Board]{.ul}](https://glossary.istqb.org/en/search/confirmation); [[SS bug fix example]{.ul}](https://github.com/nmfs-stock-synthesis/stock-synthesis/issues/148)

